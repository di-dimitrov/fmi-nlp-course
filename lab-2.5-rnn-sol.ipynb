{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fde9e435-8186-4cb3-8ec1-1be67ddb5f96</td>\n",
       "      <td>\"Трудно е класически оркестър и рок банда да с...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bb522430-40f0-4781-9910-92a1aefd013b</td>\n",
       "      <td>Следователно, Москва е пазителка на православн...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d6a84f01-9153-4f3b-bca6-ed2b2edc6a9e</td>\n",
       "      <td>От Washington Post са изготвили подробен матер...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3623488a-c528-4509-a92d-9ad4b49099ec</td>\n",
       "      <td>И пак така относно заслугите за постигнатото о...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>587b0e27-6ac8-433f-9b99-adf8d9c7c0a2</td>\n",
       "      <td>Понякога удобството да разтвориш набързо стран...</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f2d2aab0-25af-4bef-b678-badd5b390e8d</td>\n",
       "      <td>Вчера Барак Обама, отиващият си тъжен стопанин...</td>\n",
       "      <td>SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f94840f8-7fb6-42e3-aa84-0a250d58af5b</td>\n",
       "      <td>Дали защото Първият черен президент на САЩ си ...</td>\n",
       "      <td>SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b30bd670-07fd-40ac-82d7-c67988f57cc3</td>\n",
       "      <td>И като доказателство за това, гръмна и следващ...</td>\n",
       "      <td>SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>512b57e5-9b65-43a8-8dec-01bef61a3ad6</td>\n",
       "      <td>Последният път, когато Америка се обърна навът...</td>\n",
       "      <td>SUBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d6bad64f-59f2-491e-9d99-103d2748d647</td>\n",
       "      <td>Шок, бомба, ужас!</td>\n",
       "      <td>SUBJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  fde9e435-8186-4cb3-8ec1-1be67ddb5f96   \n",
       "1  bb522430-40f0-4781-9910-92a1aefd013b   \n",
       "2  d6a84f01-9153-4f3b-bca6-ed2b2edc6a9e   \n",
       "3  3623488a-c528-4509-a92d-9ad4b49099ec   \n",
       "4  587b0e27-6ac8-433f-9b99-adf8d9c7c0a2   \n",
       "5  f2d2aab0-25af-4bef-b678-badd5b390e8d   \n",
       "6  f94840f8-7fb6-42e3-aa84-0a250d58af5b   \n",
       "7  b30bd670-07fd-40ac-82d7-c67988f57cc3   \n",
       "8  512b57e5-9b65-43a8-8dec-01bef61a3ad6   \n",
       "9  d6bad64f-59f2-491e-9d99-103d2748d647   \n",
       "\n",
       "                                            sentence label  \n",
       "0  \"Трудно е класически оркестър и рок банда да с...   OBJ  \n",
       "1  Следователно, Москва е пазителка на православн...   OBJ  \n",
       "2  От Washington Post са изготвили подробен матер...   OBJ  \n",
       "3  И пак така относно заслугите за постигнатото о...   OBJ  \n",
       "4  Понякога удобството да разтвориш набързо стран...   OBJ  \n",
       "5  Вчера Барак Обама, отиващият си тъжен стопанин...  SUBJ  \n",
       "6  Дали защото Първият черен президент на САЩ си ...  SUBJ  \n",
       "7  И като доказателство за това, гръмна и следващ...  SUBJ  \n",
       "8  Последният път, когато Америка се обърна навът...  SUBJ  \n",
       "9                                  Шок, бомба, ужас!  SUBJ  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv('./data/train.csv')\n",
    "csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv['label'] = [0 if lb == 'OBJ' else 1 for lb in csv['label'] ]\n",
    "csv['label'] = csv['label'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'трудно',\n",
       " 'е',\n",
       " 'класически',\n",
       " 'оркестър',\n",
       " 'и',\n",
       " 'рок',\n",
       " 'банда',\n",
       " 'да',\n",
       " 'свирят',\n",
       " 'заедно',\n",
       " 'имаме',\n",
       " 'физически',\n",
       " 'проблеми',\n",
       " 'защото']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data = []\n",
    "for sent in csv['sentence'].values:\n",
    "  words = word_tokenize(sent)\n",
    "  words = [word.lower() for word in words if word not in string.punctuation]\n",
    "  word_data.extend(words)\n",
    "word_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(word_data)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "## use the dict to tokenize each sent\n",
    "sent_int = []\n",
    "for sent in csv['sentence'].values:\n",
    "    sent_int.append([vocab_to_int[word.lower()] for word in word_tokenize(sent) if word not in string.punctuation ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    9,  221, ...,    7,  283,   11],\n",
       "       [   0,    0,    0, ...,   40, 1312,  404],\n",
       "       [   0,    0,    0, ...,    2,  649,  405],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 5317,  999, 5318],\n",
       "       [   0,    0,    0, ...,   98, 5320,   50],\n",
       "       [   0,    0,    0, ..., 5321,    7, 1090]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 30\n",
    "def pad_sequences(sent_ints, seq_length):\n",
    "    ''' \n",
    "        Each sentence is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    sequences = np.zeros((len(sent_ints), seq_length), dtype=int)\n",
    "\n",
    "    for i, row in enumerate(sent_ints):\n",
    "        sequences[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return sequences\n",
    "pad_sequences(sent_int, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  5321\n",
      "\n",
      "Tokenized review: \n",
      " [[9, 221, 7, 1302, 1303, 2, 644, 645, 3, 1304, 402, 163, 1305, 222, 74, 52, 644, 187, 403, 2, 3, 1306, 1307, 43, 83, 145, 7, 283, 11]]\n"
     ]
    }
   ],
   "source": [
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', sent_int[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pad_sequences(sent_int, MAX_LEN)\n",
    "train_dataset = TensorDataset(torch.from_numpy(train), torch.from_numpy(csv['label'].to_numpy()))\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "  def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        return (np.array(self.x[idx], dtype=np.longlong),\n",
    "                np.array(self.y[idx], dtype=np.float32))\n",
    "train_data = RNNDataset(train, csv['label'].to_numpy())\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 20000\n",
    "MAX_LEN = 30  # cut texts after this number of words (among top max_features most common words)\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 128\n",
    "EMBEDDING_SIZE = 300\n",
    "DISPLAY_STEP = 1\n",
    "N_LAYERS = 2\n",
    "CLASSES = 1\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=EMBEDDING_SIZE)\n",
    "        self.rnn = nn.RNN(input_size=EMBEDDING_SIZE, hidden_size=HIDDEN_DIM, batch_first=True)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM, 1)\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        output, _ = self.rnn(embeddings)\n",
    "        output = output[:, -1, :]\n",
    "        return self.linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = RNN(len(vocab_to_int) +1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.2429\n",
      "Epoch 2/5, Train Loss: 0.0195\n",
      "Epoch 3/5, Train Loss: 0.0058\n",
      "Epoch 4/5, Train Loss: 0.0022\n",
      "Epoch 5/5, Train Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    for x, y  in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        if outputs.size()[0] == 1:\n",
    "            outputs = outputs.flatten() \n",
    "        else: \n",
    "            outputs = outputs.squeeze()\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(x)\n",
    "        total_samples += len(x)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}\") #Val Loss: {avg_val_loss:.4f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
